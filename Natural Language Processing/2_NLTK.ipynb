{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Tool Kit (NLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Tokenization is a process of breaking down a given paragraph of text into a list of sentence or words. When paragraph is broken down into list of sentences, it is called sentence tokenization.\n",
    "Similarly, if the sentences are further broken down into list of words, it is known as Word tokenization.\n",
    "\n",
    "Let's understand this with an example. Below is a given paragraph, let's see how tokenization works on it:\n",
    "\n",
    "\"India (Hindi: Bhārat), officially the Republic of India, is a country in South Asia. It is the seventh-largest country by area, the second-most populous country, and the most populous democracy in the world. Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west; China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east. In the Indian Ocean, India is in the vicinity of Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand and Indonesia.\"\n",
    "\n",
    "* Sentence Tokenize:\n",
    "\n",
    "    ['India (Hindi: Bhārat), officially the Republic of India, is a country in South Asia.',\n",
    "    \n",
    "  'It is the seventh-largest country by area, the second-most populous country, and the most populous democracy in the world.',\n",
    "  \n",
    "  'Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land      borders with Pakistan to the west; China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east.',\n",
    "  \n",
    "  'In the Indian Ocean, India is in the vicinity of Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand and Indonesia.']\n",
    "\n",
    "\n",
    "* Word tokenize:\n",
    "\n",
    "['India', '(', 'Hindi', ':', 'Bhārat', ')', ',', 'officially', 'the', 'Republic', 'of', 'India', ',', 'is', 'a', 'country', 'in', 'South',\n",
    " 'Asia', '.', 'It', 'is', 'the', 'seventh-largest', 'country', 'by', 'area', ',', 'the', 'second-most', 'populous', 'country', ',', 'and',\n",
    " 'the', 'most', 'populous', 'democracy', 'in', 'the', 'world', '.', 'Bounded', 'by', 'the', 'Indian',\n",
    " 'Ocean',\n",
    " 'on',\n",
    " 'the',\n",
    " 'south',\n",
    " ',',\n",
    " 'the',\n",
    " 'Arabian',\n",
    " 'Sea',\n",
    " 'on',\n",
    " 'the',\n",
    " 'southwest',\n",
    " ',',\n",
    " 'and',\n",
    " 'the',\n",
    " 'Bay',\n",
    " 'of',\n",
    " 'Bengal',\n",
    " 'on',\n",
    " 'the',\n",
    " 'southeast',\n",
    " ',',\n",
    " 'it',\n",
    " 'shares',\n",
    " 'land',\n",
    " 'borders',\n",
    " 'with',\n",
    " 'Pakistan',\n",
    " 'to',\n",
    " 'the',\n",
    " 'west',\n",
    " ';',\n",
    " 'China',\n",
    " ',',\n",
    " 'Nepal',\n",
    " ',',\n",
    " 'and',\n",
    " 'Bhutan',\n",
    " 'to',\n",
    " 'the',\n",
    " 'north',\n",
    " ';',\n",
    " 'and',\n",
    " 'Bangladesh',\n",
    " 'and',\n",
    " 'Myanmar',\n",
    " 'to',\n",
    " 'the',\n",
    " 'east',\n",
    " '.',\n",
    " 'In',\n",
    " 'the',\n",
    " 'Indian',\n",
    " 'Ocean',\n",
    " ',',\n",
    " 'India',\n",
    " 'is',\n",
    " 'in',\n",
    " 'the',\n",
    " 'vicinity',\n",
    " 'of',\n",
    " 'Sri',\n",
    " 'Lanka',\n",
    " 'and',\n",
    " 'the',\n",
    " 'Maldives',\n",
    " ';',\n",
    " 'its',\n",
    " 'Andaman',\n",
    " 'and',\n",
    " 'Nicobar',\n",
    " 'Islands',\n",
    " 'share',\n",
    " 'a',\n",
    " 'maritime',\n",
    " 'border',\n",
    " 'with',\n",
    " 'Thailand',\n",
    " 'and',\n",
    " 'Indonesia',\n",
    " '.']\n",
    "\n",
    "\n",
    "Hope this example clears up the concept of tokenization. We will understand why it is done when we will dive into text analysis.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####  Word Tokenization \n",
    "\n",
    "- Example \n",
    "- 'I am learning Natural Language processing' is being converted into \n",
    "['I', 'am', 'learning', 'Natural', 'Language', 'processing']\n",
    "\n",
    "#### Sentence Tokenization\n",
    "\n",
    "- Example \n",
    "- \"God is Great! I won a lottery.\" is bening converted into [\"God is Great!\", \"I won a lottery\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your text or import from other source\n",
    "text = 'I am learning Natural Language processing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tokenizing\n",
    "print (word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "#text = \"Good. Morning! How are you?.\"\n",
    "#text = \"Good Morning! How are you\"\n",
    "text = \" Our Company annual growth rate is 25.50%. Good job Mr.Bajaj\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text\n",
    "text = \"NLP is fun and Can deal with texts and sounds, but can't deal with images. We have session at 11AM!.We can earn lot of $\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print word by word that contains all small case and starts from samll a to z\n",
    "regexp_tokenize(text,\"[a-z]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra quote ' get's you word like can't, don't\n",
    "regexp_tokenize(text,\"[a-z']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print word by word that contains all caps and from caps A to Z\n",
    "regexp_tokenize(text,\"[A-Z]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everything in one line\n",
    "regexp_tokenize(text,\"[\\a-z']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anything starts with caret is not equal. \n",
    "regexp_tokenize(text,\"[^a-z']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only numbers\n",
    "regexp_tokenize(text,\"[0-9]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without numbers\n",
    "regexp_tokenize(text,\"[^0-9]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexp_tokenize(text,\"[$]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words\n",
    "Stop words are such words which are very common in occurrence such as ‘a’,’an’,’the’, ‘at’ etc. We ignore such words during the preprocessing part since they do not give any important information and would just take additional space. We can make our custom list of stop words as well if we want. Different libraries have different stop words list. Let’s see the stop words list for NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#If you get error download stopwords as below\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# Another way\n",
    "stopset = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stopset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding custome stopwords\n",
    "stopset.update(('new','old'))\n",
    "len(stopset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similar to the stopwords, we can also ignore punctuations in our sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import string\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords and punctuations from the above set os texts\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "punct =string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check those punctuations\n",
    "punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our text\n",
    "text = \"India (Hindi: Bhārat), officially the Republic of India, is a country in South Asia. It is the seventh-largest country by area, the second-most populous country, and the most populous democracy in the world. Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west; China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east. In the Indian Ocean, India is in the vicinity of Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand and Indonesia.\"\n",
    "\n",
    "# Empty list to load clean data\n",
    "cleaned_text = []\n",
    "\n",
    "for word in nltk.word_tokenize(text):\n",
    "    if word not in punct:\n",
    "        if word not in stop_words:\n",
    "            cleaned_text.append(word)\n",
    "    \n",
    "print ('Original Length  == >', len(text))\n",
    "print ('length of cleaned text ==>', len(cleaned_text))\n",
    "print ('\\n',cleaned_text )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into Lower case\n",
    "print (text.lower())\n",
    "\n",
    "# Convert into Upper case\n",
    "print ('\\n',text.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "- Stemming means mapping a group of words to the same stem by removing prefixes or suffixes without giving any value to the “grammatical meaning” of the stem formed after the process.\n",
    "\n",
    "e.g.\n",
    "\n",
    "computation --> comput\n",
    "\n",
    "computer --> comput \n",
    "\n",
    "hobbies --> hobbi\n",
    "\n",
    "We can see that stemming tries to bring the word back to their base word but the base word may or may not have correct grammatical meanings.\n",
    "\n",
    "There are few types of stemmers available in NLTK package. We will talk about popular below two\n",
    "- 1)\tPorter Stemmer \n",
    "- 2)\tLancaster Stemmer\n",
    "\n",
    "Let’s see how to use both of them: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.stem import PorterStemmer,LancasterStemmer,SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter stemmer\n",
      "hobbi\n",
      "hobbi\n",
      "comput\n",
      "comput\n",
      "**************************\n",
      "lancaster stemmer\n",
      "hobby\n",
      "hobby\n",
      "comput\n",
      "comput\n",
      "**************************\n"
     ]
    }
   ],
   "source": [
    "lancaster = LancasterStemmer()\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "Snowball = SnowballStemmer('english')\n",
    "\n",
    "\n",
    "print('Porter stemmer')\n",
    "print(porter.stem(\"hobby\"))\n",
    "print(porter.stem(\"hobbies\"))\n",
    "print(porter.stem(\"computer\"))\n",
    "print(porter.stem(\"computation\"))\n",
    "print(\"**************************\")  \n",
    "\n",
    "print('lancaster stemmer')\n",
    "print(lancaster.stem(\"hobby\"))\n",
    "print(lancaster.stem(\"hobbies\"))\n",
    "print(lancaster.stem(\"computer\"))\n",
    "print(lancaster.stem(\"computation\"))\n",
    "print(\"**************************\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'was',\n",
       " 'going',\n",
       " 'to',\n",
       " 'the',\n",
       " 'office',\n",
       " 'on',\n",
       " 'my',\n",
       " 'bike',\n",
       " 'when',\n",
       " 'i',\n",
       " 'saw',\n",
       " 'a',\n",
       " 'car',\n",
       " 'passing',\n",
       " 'by',\n",
       " 'hit',\n",
       " 'the',\n",
       " 'tree',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see with a new sentence\n",
    "\n",
    "sentence = \"I was going to the office on my bike when i saw a car passing by hit the tree.\"\n",
    "\n",
    "token = list(nltk.word_tokenize(sentence))\n",
    "\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I was going to the office on my bike when i saw a car passing by hit the tree.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i was go to the offic on my bike when i saw a car pass by hit the tree .\n",
      "i was going to the off on my bik when i saw a car pass by hit the tre .\n",
      "I wa go to the offic on my bike when i saw a car pass by hit the tree .\n"
     ]
    }
   ],
   "source": [
    "for stemmer in (Snowball, lancaster, porter):\n",
    "    stemm = [stemmer.stem(t) for t in token]\n",
    "    print(\" \".join(stemm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lancaster algorithm is faster than porter but it is more complex.\n",
    "Porter stemmer is the oldest algorithm present and was the most popular to use.\n",
    "\n",
    "Snowball stemmer, also known as  porter2, is the updated version of the Porter stemmer and is currently the most popular stemming algorithm.\n",
    "\n",
    "Snowball stemmer is available for multiple languages as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "run\n",
      "ran\n"
     ]
    }
   ],
   "source": [
    "# one more simple example of porter\n",
    "print(porter.stem(\"running\"))\n",
    "print(porter.stem(\"runs\"))\n",
    "print(porter.stem(\"ran\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "\n",
    "Lemmatization also does the same thing as stemming and try to bring a word to its base form, but unlike stemming it do keep in account the actual meaning of the base word i.e. the base word belongs to any specific language. The ‘base word’ is known as ‘Lemma’.\n",
    "\n",
    "We use WordNet Lemmatizer for Lemmatization in nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "run\n",
      "ran\n"
     ]
    }
   ],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "print(lemma.lemmatize('running'))\n",
    "print(lemma.lemmatize('runs'))\n",
    "print(lemma.lemmatize('ran'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see the lemma has changed for the words with same base. \n",
    "\n",
    "This is because, we haven’t given any context to the Lemmatizer.\n",
    "\n",
    "Generally, it is given by passing the POS tags for the words in a sentence.\n",
    "e.g.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "print(lemma.lemmatize('running',pos='v'))\n",
    "print(lemma.lemmatize('runs',pos='v'))\n",
    "print(lemma.lemmatize('ran',pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizer is very complex and takes a lot of time to calculate.\n",
    "\n",
    "So, it should only when the real meaning of words or the context is necessary for processing, else stemming should be preferred.\n",
    "\n",
    "It completely depends on the type of problem you are trying to solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming for Bring is bring\n",
      "Stemming for King is king\n",
      "Stemming for Going is go\n",
      "Stemming for Anything is anyth\n",
      "Stemming for Sing is sing\n",
      "Stemming for Ring is ring\n",
      "Stemming for Nothing is noth\n",
      "Stemming for Thing is thing\n"
     ]
    }
   ],
   "source": [
    "# One more example using both stemming and lemma\n",
    "#text = \"studies studying cries cry\"\n",
    "text = \"Bring King Going Anything Sing Ring Nothing Thing\"\n",
    "\n",
    "# Stemming\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer  = PorterStemmer()\n",
    "\n",
    "tokenization = nltk.word_tokenize(text)\n",
    "\n",
    "for w in tokenization:\n",
    "    print (\"Stemming for {} is {}\".format(w, porter_stemmer.stem(w))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for Bring is Bring\n",
      "Lemma for King is King\n",
      "Lemma for Going is Going\n",
      "Lemma for Anything is Anything\n",
      "Lemma for Sing is Sing\n",
      "Lemma for Ring is Ring\n",
      "Lemma for Nothing is Nothing\n",
      "Lemma for Thing is Thing\n"
     ]
    }
   ],
   "source": [
    "# Lemma \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokenization = nltk.word_tokenize(text)\n",
    "\n",
    "for w in tokenization:\n",
    "    print(\"Lemma for {} is {}\".format(w, wordnet_lemmatizer.lemmatize(w))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization takes more time to give the response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordnet\n",
    "\n",
    "- Wordnet is an NLTK corpus reader, a lexical database for English. It can be used to find the meaning of words, synonym or antonym. One can define it as a semantically oriented dictionary of English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms => {'combat-ready', 'active_voice', 'active', 'alive', 'fighting', 'dynamic', 'active_agent', 'participating'}\n",
      "Antonyms => {'passive_voice', 'passive', 'dormant', 'stative', 'inactive', 'quiet', 'extinct'}\n"
     ]
    }
   ],
   "source": [
    "# Lets find sysnonms and antonyms using python code\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"active\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "\n",
    "print('Synonyms =>',set(synonyms))\n",
    "print('Antonyms =>',set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
